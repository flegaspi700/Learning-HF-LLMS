{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620e91fa",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sentiment Analysis (SST-2)\n",
    "\n",
    "Complete pipeline for fine-tuning BERT on the Stanford Sentiment Treebank dataset. This notebook demonstrates the full workflow from data loading to model evaluation.\n",
    "\n",
    "## Workflow Overview\n",
    "- **Import Libraries** - Essential Hugging Face and PyTorch components\n",
    "- **Load SST-2 Dataset** - Using `load_dataset(\"glue\", \"sst2\")`\n",
    "- **Explore Data** - Examine structure, examples, and label distribution\n",
    "- **Tokenization** - Process single sentences (simpler than sentence pairs)\n",
    "- **Data Preparation** - Set up padding and batching\n",
    "- **Model Setup** - Configure BERT for sentiment classification\n",
    "- **Training** - Fine-tune with automatic evaluation\n",
    "- **Testing** - Evaluate on custom movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915accef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 1: Import Libraries\n",
    "\n",
    "All necessary Hugging Face and PyTorch components for the fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc91f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW  #Core PyTorch for training\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, # Text preprocessing following project patterns\n",
    "    AutoModelForSequenceClassification, # BERT with classification head\n",
    "    DataCollatorWithPadding, # Dynamic padding for batches\n",
    "    TrainingArguments, # Training configuration\n",
    "    Trainer # High-level training API\n",
    ")\n",
    "\n",
    "from datasets import load_dataset # Standardized datasets\n",
    "import evaluate  # Required for computing official GLUE metrics (accuracy for SST-2)\n",
    "import numpy as np  # Needed for processing predictions (argmax operations, array comparisons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3003d",
   "metadata": {},
   "source": [
    "# Step 2: Load and Explore the SST-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729d332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SST-2 Dataset - Using load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bcad89",
   "metadata": {},
   "source": [
    "# Step 3: Load BERT Model and Tokenizer\n",
    "\n",
    "Using bert-base-uncased for sequence classification, optimized for sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6f90fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST-2 Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Explore Data - Examine structure, examples, and label distribution\n",
    "print(\"SST-2 Dataset Structure:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# The dataset contains:\n",
    "# - Training set: ~67K sentences\n",
    "# - Validation set: ~872 sentences  \n",
    "# - Test set: ~1.8K sentences (no labels)\n",
    "# Each example has: sentence, label (0=negative, 1=positive), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8e03fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Training Examples:\n",
      "Example 0: hide new secretions from the parental units \n",
      "Label: 0 (negative)\n",
      "\n",
      "\n",
      "Example 1: contains no wit , only labored gags \n",
      "Label: 0 (negative)\n",
      "\n",
      "\n",
      "Example 2: that loves its characters and communicates something rather beautiful about human nature \n",
      "Label: 1 (positive)\n",
      "\n",
      "\n",
      "Dataset Features:\n",
      "{'sentence': Value('string'), 'label': ClassLabel(names=['negative', 'positive']), 'idx': Value('int32')}\n",
      "\n",
      "Dataset sizes:\n",
      "Training: 67349 examples\n",
      "Validation: 872 examples\n"
     ]
    }
   ],
   "source": [
    "#access training and validation sets\n",
    "train_dataset = raw_datasets['train']\n",
    "val_dataset = raw_datasets['validation']\n",
    "\n",
    "# Quick look at some examples\n",
    "print(\"\\nSample Training Examples:\")\n",
    "for i in range(3):\n",
    "    example = train_dataset[i]\n",
    "    label_text = \"positive\" if example['label'] == 1 else \"negative\"\n",
    "    print(f\"Example {i}: {example['sentence']}\")\n",
    "    print(f\"Label: {example['label']} ({label_text})\\n\")\n",
    "    print()\n",
    "\n",
    "# Check dataset features\n",
    "print(\"Dataset Features:\")\n",
    "print(train_dataset.features)\n",
    "\n",
    "# Key difference from MRPC: single 'sentence' field instead of 'sentence1' + 'sentence2'\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Training: {len(train_dataset)} examples\")\n",
    "print(f\"Validation: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d238f8",
   "metadata": {},
   "source": [
    "# Step 4: Tokenize the Dataset\n",
    "\n",
    "Converting text data into BERT-compatible format with proper padding and truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f3e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample sentence: hide new secretions from the parental units \n",
      "Tokenized input IDs: tensor([[  101,  5342,  2047,  3595,  8496,  2013,  1996, 18643,  3197,   102]])\n",
      "Tokenized attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Tokens:\n",
      " 0: [CLS]\n",
      " 1: hide\n",
      " 2: new\n",
      " 3: secret\n",
      " 4: ##ions\n",
      " 5: from\n",
      " 6: the\n",
      " 7: parental\n",
      " 8: units\n",
      " 9: [SEP]\n",
      "\n",
      "Tokenized input IDs: tensor([[  101,  5342,  2047,  3595,  8496,  2013,  1996, 18643,  3197,   102]])\n",
      "Tokenized attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Tokens:\n",
      " 0: [CLS]\n",
      " 1: hide\n",
      " 2: new\n",
      " 3: secret\n",
      " 4: ##ions\n",
      " 5: from\n",
      " 6: the\n",
      " 7: parental\n",
      " 8: units\n",
      " 9: [SEP]\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer using Hugging Face model hub identifier\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Explore how tokenization works on single sentences\n",
    "sample_sentence = train_dataset[0]['sentence']\n",
    "print(f\"\\nSample sentence: {sample_sentence}\")\n",
    "\n",
    "# Tokenization - Process single sentences (much simpler than pairs)\n",
    "inputs = tokenizer(sample_sentence, return_tensors=\"pt\")\n",
    "print(f\"Tokenized input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Tokenized attention mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# convert back to tokens to verify\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"Tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{i:2d}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ee9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "\n",
      "Columns in tokenized dataset: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "Example tokenized fields:\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_ids': [101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Example tokenized fields: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "#Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "# apply to all dataset splits using batched processing for efficiency\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the tokenized dataset structure\n",
    "print(\"\\nTokenized Dataset Structure:\")\n",
    "print(tokenized_datasets)\n",
    "print(f\"\\nColumns in tokenized dataset: {tokenized_datasets['train'].column_names}\")\n",
    "print()\n",
    "# Example tokenized fields\n",
    "print(\"Example tokenized fields:\")\n",
    "print(tokenized_datasets['train'][0])\n",
    "print(f\"\\nExample tokenized fields: {list(tokenized_datasets['train'][0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c5b5f",
   "metadata": {},
   "source": [
    "# Step 5: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2059ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence lengths:\n",
      "[10, 11, 15, 10, 22, 13, 29, 6]\n",
      "Min length: 6, Max length: 29\n",
      "\n",
      "Batch shapes after padding:\n",
      "input_ids: torch.Size([8, 29])\n",
      "token_type_ids: torch.Size([8, 29])\n",
      "attention_mask: torch.Size([8, 29])\n",
      "labels: torch.Size([8])\n",
      "\n",
      "Padded to length: 29\n"
     ]
    }
   ],
   "source": [
    "# Create data collator for dynamic padding (project convention for efficiency)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#get a batch to see how dynamic padding works\n",
    "# Remove non-tensor columns (strings can't be converted to tensors)\n",
    "samples = tokenized_datasets['train'][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence\"]}\n",
    "\n",
    "# Show original sequence lengths before padding\n",
    "print(\"Original sequence lengths:\")\n",
    "lengths = [len(x) for x in samples[\"input_ids\"]]\n",
    "print(lengths)\n",
    "print(f\"Min length: {min(lengths)}, Max length: {max(lengths)}\")\n",
    "\n",
    "# Apply data collator to create a batch with dynamic padding\n",
    "batch = data_collator(samples)\n",
    "\n",
    "# Show shapes after padding - all sequences now have same length\n",
    "print(\"\\nBatch shapes after padding:\")\n",
    "for key, tensor in batch.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "\n",
    "# All sequences are now padded to the same length (longest in the batch)\n",
    "print(f\"\\nPadded to length: {batch['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddd6e5",
   "metadata": {},
   "source": [
    "# Step 6: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d143f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded successfully.\n",
      "Model has parameters: 109,483,778\n",
      "Metrics computation function configured!\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model with classification head for sentiment analysis\n",
    "# Using Hugging Face model hub identifier (project convention)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2,  # SST-2 has 2 classes: positive and negative\n",
    "    #torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "    device_map=\"auto\"  # Automatically place model on available devices (CPU/GPU/TPU)\n",
    ")\n",
    "\n",
    "print(\" Model loaded successfully.\")\n",
    "print(f\"Model has parameters: {model.num_parameters():,}\")\n",
    "\n",
    "#set up metrics computation for SST-2 (accuracy)\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Compute accuracy for SST-2 task during training.\n",
    "    Called automatically by the Trainer.\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    \n",
    "    # Convert logits to predictions using argmax\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Return computed metrics (accuracy for SST-2)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Example data during evaluation\n",
    "# logits = np.array([\n",
    "#    [-0.5, 2.1],   # Model thinks: negative=-0.5, positive=2.1 → predicts positive (1)\n",
    "#    [1.8, -0.3],   # Model thinks: negative=1.8, positive=-0.3 → predicts negative (0)\n",
    "#    [-1.2, 0.9]    # Model thinks: negative=-1.2, positive=0.9 → predicts positive (1)\n",
    "# ])\n",
    "# labels = np.array([1, 0, 1])  # True labels: positive, negative, positive\n",
    "\n",
    "# predictions = np.argmax(logits, axis=-1)  # Result: [1, 0, 1]\n",
    "# Accuracy: 3/3 = 100% (all predictions match true labels)\n",
    "\n",
    "print(\"Metrics computation function configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75076b0",
   "metadata": {},
   "source": [
    "# Step 7: Fine-Tune the Model\n",
    "\n",
    "Training BERT on SST-2 using the Trainer API. This process adapts the pre-trained model to our specific sentiment classification task.\n",
    "\n",
    "**Training Progress:**\n",
    "- **Epoch 1:** Starting loss ~0.69, accuracy improves rapidly\n",
    "- **Epoch 2:** Continued improvement with better convergence  \n",
    "- **Epoch 3:** Final refinement, achieving high accuracy\n",
    "\n",
    "**Key Configuration:**\n",
    "- **BF16 Precision:** Enables stable mixed-precision training\n",
    "- **Learning Rate:** 2e-5 provides optimal convergence for BERT\n",
    "- **Batch Sizes:** Balanced for GPU memory and training stability\n",
    "\n",
    "**Expected Results:**\n",
    "- Training should converge to >90% accuracy\n",
    "- Validation loss should decrease consistently\n",
    "- Model learns to distinguish positive/negative sentiment effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfe5cc",
   "metadata": {},
   "source": [
    "## Training Parameters Configuration\n",
    "\n",
    "| Parameter | Your Value | Standard Range | Notes |\n",
    "|-----------|------------|----------------|-------|\n",
    "| **num_train_epochs** | 3 | 2-5 | ✅ Perfect for fine-tuning |\n",
    "| **train_batch_size** | 16 | 8-32 | ✅ Good for most hardware |\n",
    "| **eval_batch_size** | 64 | 32-128 | ✅ Efficient eval batching |\n",
    "| **warmup_steps** | 500 | 100-1000 | ✅ Standard for BERT |\n",
    "| **weight_decay** | 0.01 | 0.01-0.1 | ✅ Common choice |\n",
    "\n",
    "## Why Keep bf16=True and Remove torch_dtype=torch.float16\n",
    "**Training Precision vs Model Precision**  \n",
    "Following your project's device management patterns:\n",
    "\n",
    "- **bf16=True**: Controls how training computations are done (gradients, forward/backward passes)\n",
    "- **torch_dtype=torch.float16**: Controls how model weights are stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5347a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured!\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments following project patterns\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sst2-bert-finetuned\",      # Model checkpoint directory\n",
    "    eval_strategy=\"epoch\",                   # Evaluate at end of each epoch\n",
    "    save_strategy=\"epoch\",                   # Save model at end of each epoch\n",
    "    logging_dir=\"./logs\",                    # Directory for storing logs\n",
    "    num_train_epochs=3,                      # Number of training epochs\n",
    "    per_device_train_batch_size=16,          # Batch size for training\n",
    "    per_device_eval_batch_size=64,           # Batch size for evaluation\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,                        # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                       # Strength of weight decay\n",
    "    logging_steps=100,                       # Log every 100 steps\n",
    "    load_best_model_at_end=True,            # Load best model when training ends\n",
    "    metric_for_best_model=\"accuracy\",        # Use accuracy to determine best model\n",
    "    greater_is_better=True,                  # Higher accuracy is better\n",
    "    #fp16=True,                              # Use float16 for efficiency (project convention)\n",
    "    bf16=True,                                # Use BF16 instead - more stable for training\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2103a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured and ready!\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer object following project patterns\n",
    "trainer = Trainer(\n",
    "    model=model,                                    # The model to train\n",
    "    args=training_args,                            # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],     # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # Validation dataset\n",
    "    data_collator=data_collator,                   # Data collator for padding\n",
    "    processing_class=tokenizer,                    # Tokenizer for processing\n",
    "    compute_metrics=compute_metrics,               # Function to compute metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer configured and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31abbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12630/12630 11:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.241284</td>\n",
       "      <td>0.920872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.329299</td>\n",
       "      <td>0.912844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.356599</td>\n",
       "      <td>0.920872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed!\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning with automatic evaluation\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning completed!\")\n",
    "\n",
    "# The trainer will automatically:\n",
    "# - Run training for specified epochs\n",
    "# - Evaluate on validation set after each epoch\n",
    "# - Log training loss and accuracy metrics\n",
    "# - Save the best performing model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e3e99",
   "metadata": {},
   "source": [
    "# Step 8: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc6edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on custom movie reviews\n",
    "# Following project patterns for interactive prompting\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text using the fine-tuned model.\n",
    "    Returns prediction and confidence scores.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Move inputs to same device as model\n",
    "    if hasattr(model, 'device'):\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    # Convert to probabilities using softmax\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get prediction (0=negative, 1=positive)\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return prediction, confidence, probabilities[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f471eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fine-tuned BERT on custom movie reviews:\n",
      "============================================================\n",
      "\n",
      "Test 1:\n",
      "Review: This movie was absolutely fantastic! Amazing storyline and great acting.\n",
      "Prediction: Positive (confidence: 0.999)\n",
      "Probabilities: Negative=0.001, Positive=0.999\n",
      "\n",
      "Test 2:\n",
      "Review: Terrible film. Boring plot and bad acting throughout.\n",
      "Prediction: Negative (confidence: 0.999)\n",
      "Probabilities: Negative=0.999, Positive=0.001\n",
      "\n",
      "Test 3:\n",
      "Review: The movie was okay, nothing special but not terrible either.\n",
      "Prediction: Positive (confidence: 0.738)\n",
      "Probabilities: Negative=0.262, Positive=0.738\n",
      "\n",
      "Test 4:\n",
      "Review: One of the worst movies I've ever seen. Complete waste of time.\n",
      "Prediction: Negative (confidence: 0.998)\n",
      "Probabilities: Negative=0.998, Positive=0.002\n"
     ]
    }
   ],
   "source": [
    "# Test with custom movie reviews following project's interactive patterns\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! Amazing storyline and great acting.\",\n",
    "    \"Terrible film. Boring plot and bad acting throughout.\",\n",
    "    \"The movie was okay, nothing special but not terrible either.\",\n",
    "    \"One of the worst movies I've ever seen. Complete waste of time.\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned BERT on custom movie reviews:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    prediction, confidence, probs = predict_sentiment(review, model, tokenizer)\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    \n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Probabilities: Negative={probs[0]:.3f}, Positive={probs[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c69902fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Interactive Testing - Enter your own movie review:\n",
      "Your review: The movie was sad.\n",
      "Model prediction: Negative (confidence: 0.998)\n",
      "Probabilities: Negative=0.998, Positive=0.002\n",
      "Your review: The movie was sad.\n",
      "Model prediction: Negative (confidence: 0.998)\n",
      "Probabilities: Negative=0.998, Positive=0.002\n",
      "Testing completed!\n",
      "Testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - following project patterns for user input\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Interactive Testing - Enter your own movie review:\")\n",
    "\n",
    "while True:\n",
    "    user_review = input(\"\\nEnter a movie review (or 'quit' to exit): \")\n",
    "    \n",
    "    if user_review.lower() in ['quit', 'exit', 'q']:\n",
    "        break\n",
    "    \n",
    "    if user_review.strip():\n",
    "        prediction, confidence, probs = predict_sentiment(user_review, model, tokenizer)\n",
    "        sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "        \n",
    "        print(f\"Your review: {user_review}\")\n",
    "        print(f\"Model prediction: {sentiment} (confidence: {confidence:.3f})\")\n",
    "        print(f\"Probabilities: Negative={probs[0]:.3f}, Positive={probs[1]:.3f}\")\n",
    "    else:\n",
    "        print(\"Please enter a valid review.\")\n",
    "\n",
    "print(\"Testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6f29b",
   "metadata": {},
   "source": [
    "## Key Differences from Base BERT\n",
    "\n",
    "| Aspect | Original BERT | Our Fine-tuned Model |\n",
    "|--------|---------------|---------------------|\n",
    "| **Task** | General language understanding | Movie sentiment classification |\n",
    "| **Output** | Masked word predictions | Positive/Negative sentiment |\n",
    "| **Training Data** | Wikipedia, books (general text) | Movie reviews with sentiment labels |\n",
    "| **Specialization** | Broad language knowledge | Domain-specific sentiment analysis |\n",
    "| **Model Architecture** | BERT encoder + MLM head | BERT encoder + classification head |\n",
    "| **Performance Focus** | Language modeling accuracy | Sentiment classification accuracy |\n",
    "| **Use Case** | Foundation for downstream tasks | Ready-to-use sentiment analyzer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092edcc",
   "metadata": {},
   "source": [
    "# Step 9: Interactive Testing Function\n",
    "\n",
    "This function allows you to test any sentence with the fine-tuned model and see:\n",
    "- **Predicted sentiment** (Positive/Negative)\n",
    "- **Confidence score** (0-1 scale)\n",
    "- **Model reasoning** through raw prediction probabilities\n",
    "\n",
    "Try different types of sentences to see how well the model performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097896f2",
   "metadata": {},
   "source": [
    "# Step 10: Summary and Next Steps\n",
    "\n",
    "**What we accomplished:**\n",
    "- Successfully fine-tuned BERT for sentiment analysis on SST-2 dataset\n",
    "- Achieved high accuracy (>90%) on movie review sentiment classification\n",
    "- Created an interactive testing function for real-time predictions\n",
    "\n",
    "**Key learnings:**\n",
    "- BF16 precision enables stable training with faster computation\n",
    "- Proper tokenization and data preparation are crucial for good results\n",
    "- The Trainer API simplifies the fine-tuning process significantly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
