{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efd22eb",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sequence Classification\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained BERT model for sequence classification using the Hugging Face transformers library. We'll work with the GLUE MRPC (Microsoft Research Paraphrase Corpus) dataset to classify whether two sentences are paraphrases of each other.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the basic fine-tuning workflow\n",
    "- Learn how to preprocess data for BERT\n",
    "- Use the Trainer API for training and evaluation\n",
    "- Compute metrics to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch libraries for training\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Hugging Face transformers for pre-trained models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb6b3f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import the essential libraries we'll need for fine-tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Create sample training data (positive sentiment examples)\n",
    "sequences = [\n",
    "    \"This is a great movie!\",\n",
    "    \"This is a fantastic movie!\",\n",
    "]\n",
    "\n",
    "# Tokenize the input sequences\n",
    "# padding=True ensures all sequences have the same length\n",
    "# truncation=True cuts off sequences that are too long\n",
    "# return_tensors=\"pt\" returns PyTorch tensors\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Add labels for our examples (1 = positive, 0 = negative)\n",
    "batch[\"labels\"] = torch.tensor([1, 1])  \n",
    "\n",
    "# Set up optimizer (AdamW is commonly used for transformers)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Forward pass: compute loss\n",
    "loss = model(**batch).loss\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass: compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update model parameters\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Completed one training step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf9681",
   "metadata": {},
   "source": [
    "## 2. Basic Fine-Tuning Example\n",
    "\n",
    "Let's start with a simple example to understand the basic workflow of fine-tuning. We'll create a minimal training loop with just two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GLUE MRPC dataset using Hugging Face datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Display the dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# The dataset contains:\n",
    "# - Training set: 3,668 sentence pairs\n",
    "# - Validation set: 408 sentence pairs  \n",
    "# - Test set: 1,725 sentence pairs\n",
    "# Each example has: sentence1, sentence2, label (0=not paraphrase, 1=paraphrase), idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8767b0d",
   "metadata": {},
   "source": [
    "## 3. Loading a Real Dataset\n",
    "\n",
    "Now let's work with a real dataset from the GLUE benchmark. We'll use the MRPC (Microsoft Research Paraphrase Corpus) dataset which contains pairs of sentences labeled as paraphrases or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .',\n",
       " 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .',\n",
       " 'label': 0,\n",
       " 'idx': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the training dataset and examine a specific example\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "# Look at example 14 from the training set\n",
    "example = raw_train_dataset[14]\n",
    "print(\"Example from training set:\")\n",
    "print(f\"Sentence 1: {example['sentence1']}\")\n",
    "print(f\"Sentence 2: {example['sentence2']}\")\n",
    "print(f\"Label: {example['label']} ({'Paraphrase' if example['label'] == 1 else 'Not paraphrase'})\")\n",
    "print(f\"Index: {example['idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb1960",
   "metadata": {},
   "source": [
    "### 3.1 Exploring the Dataset\n",
    "\n",
    "Let's examine the structure and content of our dataset to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da7e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value('string'),\n",
       " 'sentence2': Value('string'),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
       " 'idx': Value('int32')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the features (columns) in the training dataset\n",
    "print(\"Training dataset features:\")\n",
    "print(raw_train_dataset.features)\n",
    "\n",
    "# This shows us the data types and structure:\n",
    "# - sentence1: string\n",
    "# - sentence2: string  \n",
    "# - label: ClassLabel with 2 classes (0, 1)\n",
    "# - idx: int32 (unique identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'However , EPA officials would not confirm the 20 percent figure .',\n",
       " 'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .',\n",
       " 'label': 0,\n",
       " 'idx': 812}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at an example from the validation set\n",
    "raw_validation_dataset = raw_datasets[\"validation\"]\n",
    "\n",
    "example_val = raw_validation_dataset[87]\n",
    "print(\"Example from validation set:\")\n",
    "print(f\"Sentence 1: {example_val['sentence1']}\")\n",
    "print(f\"Sentence 2: {example_val['sentence2']}\")\n",
    "print(f\"Label: {example_val['label']} ({'Paraphrase' if example_val['label'] == 1 else 'Not paraphrase'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c5d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value('string'),\n",
       " 'sentence2': Value('string'),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
       " 'idx': Value('int32')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset has the same structure as training dataset\n",
    "print(\"Validation dataset features:\")\n",
    "print(raw_validation_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd03e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 24049,  2001,  2087,  3728,  3026,  3580,  2343,  2005,  1996,\n",
       "          9722,  1004,  4132,  9340, 12439,  2964,  2449,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the first sentence from example 15\n",
    "sentence1 = raw_datasets[\"train\"][\"sentence1\"][15]\n",
    "print(f\"Original sentence 1: {sentence1}\")\n",
    "\n",
    "inputs = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "print(f\"Tokenized input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cc5b1",
   "metadata": {},
   "source": [
    "## 4. Tokenization for BERT\n",
    "\n",
    "Now we need to convert our text data into tokens that BERT can understand. Let's explore how BERT tokenizes single sentences and sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783a46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3026,  3580,  2343,  4388, 24049,  1010,  3839,  2132,  1997,\n",
       "          1996,  9722,  1998,  4132,  9340, 12439,  2964,  3131,  1010,  2097,\n",
       "          2599,  1996,  2047,  9178,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the second sentence from the same example\n",
    "sentence2 = raw_datasets[\"train\"][\"sentence2\"][15]\n",
    "print(f\"Original sentence 2: {sentence2}\")\n",
    "\n",
    "inputs = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "print(f\"Tokenized input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ca713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 24049,  2001,  2087,  3728,  3026,  3580,  2343,  2005,  1996,\n",
       "          9722,  1004,  4132,  9340, 12439,  2964,  2449,  1012,   102,  3026,\n",
       "          3580,  2343,  4388, 24049,  1010,  3839,  2132,  1997,  1996,  9722,\n",
       "          1998,  4132,  9340, 12439,  2964,  3131,  1010,  2097,  2599,  1996,\n",
       "          2047,  9178,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize both sentences together (this is what we need for sentence pair tasks)\n",
    "# BERT uses [CLS] token at start, [SEP] to separate sentences, and [SEP] at end\n",
    "sentence1 = raw_datasets[\"train\"][\"sentence1\"][15]\n",
    "sentence2 = raw_datasets[\"train\"][\"sentence2\"][15]\n",
    "\n",
    "inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\")\n",
    "print(f\"Combined tokenization:\")\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Token type IDs: {inputs['token_type_ids']}\")  # 0 for sentence1, 1 for sentence2\n",
    "print(f\"Attention mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'rudder',\n",
       " 'was',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'for',\n",
       " 'the',\n",
       " 'developer',\n",
       " '&',\n",
       " 'platform',\n",
       " 'evan',\n",
       " '##gel',\n",
       " '##ism',\n",
       " 'business',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'eric',\n",
       " 'rudder',\n",
       " ',',\n",
       " 'formerly',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'developer',\n",
       " 'and',\n",
       " 'platform',\n",
       " 'evan',\n",
       " '##gel',\n",
       " '##ism',\n",
       " 'unit',\n",
       " ',',\n",
       " 'will',\n",
       " 'lead',\n",
       " 'the',\n",
       " 'new',\n",
       " 'entity',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert token IDs back to readable tokens to see how BERT processes the text\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(\"Tokens with special BERT tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{i:2d}: {token}\")\n",
    "\n",
    "# Notice: [CLS] at start, [SEP] between sentences, [SEP] at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35645df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5677148366541f69e1fdbef32afd6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to tokenize sentence pairs\n",
    "def tokenize_function(example):\n",
    "    # tokenizer automatically handles sentence pairs when given two arguments\n",
    "    # truncation=True ensures sequences don't exceed BERT's max length (512 tokens)\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Apply tokenization to all splits of the dataset\n",
    "# batched=True processes multiple examples at once for efficiency\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenized dataset structure:\")\n",
    "print(tokenized_dataset)\n",
    "print(f\"\\nExample tokenized fields: {list(tokenized_dataset['train'][0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1508a2",
   "metadata": {},
   "source": [
    "### 4.1 Batch Tokenization\n",
    "\n",
    "Now let's tokenize the entire dataset efficiently using the `map` function. This applies our tokenization function to all examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35645df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a data collator that will pad sequences to the same length in each batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Get a small sample to demonstrate padding\n",
    "# Remove non-tensor columns (strings can't be converted to tensors)\n",
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "\n",
    "# Show the original lengths before padding\n",
    "print(\"Original sequence lengths:\")\n",
    "lengths = [len(x) for x in samples[\"input_ids\"]]\n",
    "print(lengths)\n",
    "print(f\"Min length: {min(lengths)}, Max length: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351545d",
   "metadata": {},
   "source": [
    "## 5. Data Collation and Padding\n",
    "\n",
    "Since sequences have different lengths, we need to pad them to the same length for batching. The `DataCollatorWithPadding` handles this dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the data collator to create a properly padded batch\n",
    "batch = data_collator(samples)\n",
    "\n",
    "# Show the shapes after padding - all sequences now have the same length\n",
    "print(\"Batch shapes after padding:\")\n",
    "for key, tensor in batch.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "\n",
    "# All sequences are now padded to the same length (the longest in the batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c61d7",
   "metadata": {},
   "source": [
    " Replicate the preprocessing on the GLUE SST-2 dataset. It’s a little bit different since it’s composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define basic training configuration\n",
    "# \"test-trainer\" is the output directory where model checkpoints will be saved\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-trainer\",\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d6977",
   "metadata": {},
   "source": [
    "## 6. Setting Up Training with the Trainer API\n",
    "\n",
    "The Hugging Face `Trainer` class provides a high-level interface for training and evaluation. Let's set up the training arguments and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c754f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load BERT with a classification head for 2 classes (paraphrase vs not paraphrase)\n",
    "# num_labels=2 adds a classification layer on top of BERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b901103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer object\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                              # The model to train\n",
    "    args=training_args,                       # Training arguments\n",
    "    train_dataset=tokenized_dataset[\"train\"], # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # Validation dataset\n",
    "    data_collator=data_collator,              # Data collator for padding\n",
    "    processing_class=tokenizer,               # Tokenizer for processing\n",
    ")\n",
    "\n",
    "print(\"Trainer configured and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86787959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.322900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.36212434124582993, metrics={'train_runtime': 110.8924, 'train_samples_per_second': 99.231, 'train_steps_per_second': 12.417, 'total_flos': 405114969714960.0, 'train_loss': 0.36212434124582993, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training! This will fine-tune BERT on the MRPC dataset\n",
    "# The trainer will automatically handle the training loop, loss computation, and optimization\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f34439",
   "metadata": {},
   "source": [
    "### 6.1 Training the Model\n",
    "\n",
    "Now let's actually train the model! This will take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aded71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the validation set\n",
    "predictions = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "\n",
    "print(f\"Prediction logits shape: {predictions.predictions.shape}\")  # (num_examples, num_classes)\n",
    "print(f\"True labels shape: {predictions.label_ids.shape}\")         # (num_examples,)\n",
    "print(f\"Number of validation examples: {len(predictions.label_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70320df5",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Now let's evaluate our trained model on the validation set to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to predicted class labels\n",
    "import numpy as np\n",
    "\n",
    "# Take the argmax to get the predicted class (0 or 1)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "print(f\"First 10 predictions: {preds[:10]}\")\n",
    "print(f\"First 10 true labels: {predictions.label_ids[:10]}\")\n",
    "\n",
    "# Quick accuracy calculation\n",
    "accuracy = (preds == predictions.label_ids).mean()\n",
    "print(f\"Simple accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a7164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e78f0a1f02b48e1a689f3b5d2407fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8504901960784313, 'f1': 0.893542757417103}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the official GLUE metric for MRPC\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "# Compute the official metrics (accuracy and F1 score)\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "print(\"Official GLUE MRPC metrics:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# MRPC uses both accuracy and F1 score as evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33e00d",
   "metadata": {},
   "source": [
    "### 7.1 Computing Proper Metrics\n",
    "\n",
    "Let's use the official GLUE metrics for proper evaluation of our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that computes metrics during training\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Function to compute metrics during evaluation.\n",
    "    Called automatically by the Trainer.\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Return the computed metrics\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"Metric computation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585e81b",
   "metadata": {},
   "source": [
    "### 7.2 Setting Up Automatic Evaluation\n",
    "\n",
    "For better training, we can set up automatic evaluation during training. This helps monitor our model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dceec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create new training arguments with evaluation enabled\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-trainer\",\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # Save model at the end of each epoch\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=2,              # Fewer epochs for this demo\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    load_best_model_at_end=True,     # Load the best model when training ends\n",
    ")\n",
    "\n",
    "# Load a fresh model (this will reset any previous training)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Create new trainer with metric computation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # This enables automatic metric computation\n",
    ")\n",
    "\n",
    "print(\"New trainer with evaluation configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990c480",
   "metadata": {},
   "source": [
    "### 7.3 Training with Automatic Evaluation\n",
    "\n",
    "Now let's train a new model with evaluation happening at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8eec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430748</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.880570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.559400</td>\n",
       "      <td>0.413827</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.900685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.664447</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.900506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3823748209724066, metrics={'train_runtime': 116.2305, 'train_samples_per_second': 94.674, 'train_steps_per_second': 11.847, 'total_flos': 405114969714960.0, 'train_loss': 0.3823748209724066, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with automatic evaluation\n",
    "# You'll see both training loss and evaluation metrics after each epoch\n",
    "print(\"Starting training with evaluation...\")\n",
    "trainer.train()\n",
    "print(\"Training with evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517e40f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully fine-tuned a BERT model for sequence classification. Here's what we accomplished:\n",
    "\n",
    "1. **Basic Fine-tuning**: Understood the core workflow with a simple example\n",
    "2. **Dataset Loading**: Loaded and explored the GLUE MRPC dataset\n",
    "3. **Tokenization**: Learned how BERT processes sentence pairs\n",
    "4. **Data Preparation**: Set up efficient batching and padding\n",
    "5. **Training Setup**: Configured the Trainer API for fine-tuning\n",
    "6. **Model Training**: Fine-tuned BERT on the paraphrase detection task\n",
    "7. **Evaluation**: Computed proper metrics to assess performance\n",
    "\n",
    "### Key Takeaways\n",
    "- Pre-trained models need only the classification head to be randomly initialized\n",
    "- Tokenization for sentence pairs uses special tokens ([CLS], [SEP])\n",
    "- The Trainer API simplifies the training process significantly\n",
    "- Evaluation during training helps monitor progress\n",
    "- GLUE tasks have specific metrics (accuracy + F1 for MRPC)\n",
    "\n",
    "### Next Steps\n",
    "- Try different hyperparameters (learning rate, batch size, epochs)\n",
    "- Experiment with other GLUE tasks (SST-2, QNLI, etc.)\n",
    "- Compare different pre-trained models (RoBERTa, DeBERTa)\n",
    "- Explore parameter-efficient fine-tuning (LoRA, adapters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
