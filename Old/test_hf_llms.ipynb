{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e674b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learn\\AI-Learning\\Learning-HF-LLMS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51d0060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598045349121094},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229700f",
   "metadata": {},
   "source": [
    "# Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee346a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#use default model checkpoint\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "#padding is for making sure all inputs are the same length / shape\n",
    "#trunctation is for cutting off inputs that are too long\n",
    "#return_tensors is for returning PyTorch tensors\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796ebdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model and getting raw logits\n",
    "from transformers import AutoModel\n",
    "\n",
    "#use default model checkpoint\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc92484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe8b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Getting logits for classification\n",
    "# classify the sentences as positive or negative\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#use default model checkpoint\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pre processing the output\n",
    "# Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. \n",
    "# Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "# To be converted to probabilities, they need to go through a SoftMax layer \n",
    "# (all ðŸ¤— Transformers models output the logits, as the loss function for training will generally fuse the \n",
    "# last activation function, such as SoftMax, with the actual loss function, such as cross entropy)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41780268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "# First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "# Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b9ca7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf58317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learn\\AI-Learning\\Learning-HF-LLMS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create a transformer and saving a model locally\n",
    "from transformers import AutoModel\n",
    "\n",
    "file_path = \"D:\\\\Learn\\\\models\\\\bert-base-cased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "model.save_pretrained(file_path)\n",
    "\n",
    "# loading the model from local path\n",
    "# we're not going to run this. Just showing how to do it.\n",
    "\n",
    "# from transformers import AutoModel\n",
    "# file_path = \"D:\\\\Learn\\\\models\\\\bert-base-cased\"\n",
    "# model = AutoModel.from_pretrained(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf9e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 170, 2774, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#Transformer models handle text by turning the inputs into numbers. Here we will look at exactly what happens when your text is processed by the tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"This is a test\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a302864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This is a test [SEP]\n"
     ]
    }
   ],
   "source": [
    "# We can decode the input IDs to get back the original text\n",
    "print(tokenizer.decode(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141db9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1188, 1110, 170, 2774, 102], [101, 1188, 1110, 170, 1248, 2039, 2774, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#encode multiple inputs\n",
    "texts = [\"This is a test\", \"This is a second longer test\"]\n",
    "encoded_inputs = tokenizer(texts)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1188, 1110,  170, 2774,  102,    0,    0],\n",
      "        [ 101, 1188, 1110,  170, 1248, 2039, 2774,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Note that when passing multiple sentences, the tokenizer returns a list for each sentence for each dictionary value. \n",
    "# We can also ask the tokenizer to return tensors directly from PyTorch:\n",
    "\n",
    "# if your input sentences are of different lengths, you can use the padding\n",
    "# and truncation arguments to ensure that they are all the same length.\n",
    "encoded_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "# tokenization is the first step in the process of using a transformer model.\n",
    "# tokenization is the process of converting raw text into a format that the model can understand.\n",
    "# This usually involves splitting the text into smaller units (tokens) and mapping those tokens to numerical values (input IDs).\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"This is a test\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07399ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1188, 1110, 170, 2774]\n"
     ]
    }
   ],
   "source": [
    "# Tokens to input IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f758b0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decoding input IDs back to tokens\n",
    "# Note that the decode method not only converts the indices back to tokens, \n",
    "# but also groups together the tokens that were part of the same words to produce a readable sentence. \n",
    "# This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, \n",
    "# or for sequence-to-sequence problems like translation or summarization).\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65c5a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: ['this', 'is', 'a', 'great', 'course', ',', 'i', 'am', 'learning', 'a', 'lot', '!']\n",
      "ids: [2023, 2003, 1037, 2307, 2607, 1010, 1045, 2572, 4083, 1037, 2843, 999]\n",
      "tensor: tensor([[2023, 2003, 1037, 2307, 2607, 1010, 1045, 2572, 4083, 1037, 2843,  999]])\n",
      "logits: tensor([[-4.1740,  4.4881]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"This is a great course, I am learning a lot!\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"token:\", tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"ids:\", ids)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"tensor:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ce09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: {'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307, 2607, 1010, 1045, 2572, 4083, 1037, 2843,\n",
      "          999,  102],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2023, 2003, 1996, 5409, 2607, 1045, 2031, 2412, 2579, 1012,  102,\n",
      "            0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# batching is the process of grouping multiple inputs together to be processed at the same time.\n",
    "# This is more efficient than processing each input individually, as it allows the model to take advantage of parallelism and reduce the overhead of loading the model multiple times.\n",
    "# Batching is especially important when working with large datasets, as it can significantly speed up the training and inference process.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"This is a great course, I am learning a lot!\",\n",
    "    \"I hate this so much!\",\n",
    "    \"This is the worst course I have ever taken.\",\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"tokens:\", tokens)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
